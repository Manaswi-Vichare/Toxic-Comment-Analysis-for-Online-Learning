{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "data1 = pd.read_csv('Users/Dell/Downloads/datac.csv', header=0).fillna(' ')\n",
    "data = pd.read_csv('Users/Dell/Downloads/datab.csv', header=0).fillna(' ')\n",
    "\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.concat([data1['comment_text'], data['comment_text']]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True,\n",
    "strip_accents='unicode',\n",
    "analyzer='word',\n",
    "token_pattern=r'[a-z]{3,}',\n",
    "stop_words=\"english\",\n",
    "ngram_range=(1, 2),\n",
    "max_df=50000,\n",
    "max_features=300)\n",
    "\n",
    "tfidf.fit(text)\n",
    "train_features = tfidf.transform(data1['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data1, X_data, y_data1, y_data = train_test_split(train_features.toarray(), data1[classes], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {k:[] for k in y_data.columns.tolist()}\n",
    "plt.figure(0,figsize=(8,8)).clf()\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "avg_auc=0\n",
    "for class_ in classes:\n",
    "    logR = LogisticRegression(solver='sag', penalty='l2')\n",
    "    model = GridSearchCV(estimator=logR, cv=3, param_grid={'C': [0.01, 0.1,1,10]}, scoring='roc_auc')\n",
    "    model.fit(X_data1, y_data1[class_])\n",
    "    print('CV Score for {} = {}'.format(class_, model.best_score_))\n",
    "    prediction = model.predict_proba(X_data)\n",
    "    actual = y_data[class_]\n",
    "    fpr, tpr, threshold = roc_curve(actual, prediction[:,1])\n",
    "    d[class_] = d[class_] + np.where(prediction[:,1] >= threshold [np.argmax(tpr-fpr)],1,0).tolist()\n",
    "    AUC = np.round(roc_auc_score (actual, prediction[:,1]),2)\n",
    "    avg_auc = avg_auc + AUC\n",
    "    plt.plot(fpr, tpr, label=class_ + 'AUC =' +str(AUC))\n",
    "    plt.legend (loc= \"lower right\")\n",
    "\n",
    "plt.title('Logistic Regression with L2 Penalty and 3 Fold CV | Mean AUC = {}'.format(np.round(float(avg_auc)/6.0,2)))\n",
    "y_pred = pd.DataFrame(d)\n",
    "y_true = y_data.reset_index(drop=True)\n",
    "match = {k:object for k in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']}\n",
    "for i in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n",
    "    match[i] = np.where(y_true[i] == y_pred[i],1,0)\n",
    "\n",
    "df_match = pd.DataFrame(match)\n",
    "\n",
    "df_match['sum'] = df_match[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1)\n",
    "df_match['hamming_match'] = df_match['sum'].astype(float)/6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {k:[] for k in y_data.columns.tolist()}\n",
    "plt.figure(0,figsize=(8,8)).clf()\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "avg_auc=0\n",
    "for class_ in classes:\n",
    "    logR = RandomForestClassifier(n_estimators=1000, n_jobs, oob_score=True)\n",
    "    model.fit(X_data1, y_data1[class_])\n",
    "    prediction = model.predict_proba(X_data)\n",
    "    actual = y_data[class_]\n",
    "    fpr, tpr, threshold = roc_curve(actual, prediction[:,1])\n",
    "    d[class_] = d[class_] + np.where(prediction[:,1] >= threshold [np.argmax(tpr-fpr)],1,0).tolist()\n",
    "    AUC = np.round(roc_auc_score (actual, prediction[:,1]),2)\n",
    "    avg_auc = avg_auc + AUC\n",
    "    plt.plot(fpr, tpr, label=class_ + 'AUC =' +str(AUC))\n",
    "    plt.legend (loc= \"lower right\")\n",
    "\n",
    "plt.title('Random Forest | Mean AUC = {}'.format(np.round(float(avg_auc)/6.0,2)))\n",
    "y_pred = pd.DataFrame(d)\n",
    "y_true = y_data.reset_index(drop=True)\n",
    "match = {k:object for k in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']}\n",
    "for i in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n",
    "    match[i] = np.where(y_true[i] == y_pred[i],1,0)\n",
    "\n",
    "df_match = pd.DataFrame(match)\n",
    "\n",
    "df_match['sum'] = df_match[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1)\n",
    "df_match['hamming_match'] = df_match['sum'].astype(float)/6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hamming score = %s' % str(np.round(df_match['hamming_match'].sum(axis=0)/len(df_match),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {k:[] for k in y_data.columns.tolist()}\n",
    "plt.figure(0,figsize=(8,8)).clf()\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "avg_auc=0\n",
    "for class_ in classes:\n",
    "    print(class_)\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_data1, y_data1[class_])\n",
    "    prediction = model.predict_proba(X_data)\n",
    "    actual = y_data[class_]\n",
    "    fpr, tpr, threshold = roc_curve(actual, prediction[:,1])\n",
    "    d[class_] = d[class_] + np.where(prediction[:,1] >= threshold [np.argmax(tpr-fpr)],1,0).tolist()\n",
    "    AUC = np.round(roc_auc_score (actual, prediction[:,1]),2)\n",
    "    avg_auc = avg_auc + AUC\n",
    "    plt.plot(fpr, tpr, label=class_ + 'AUC =' +str(AUC))\n",
    "    plt.legend (loc= \"lower right\")\n",
    "\n",
    "plt.title('Multinomial NB | Mean AUC = {}'.format(np.round(float(avg_auc)/6.0,2)))\n",
    "y_pred = pd.DataFrame(d)\n",
    "y_true = y_data.reset_index(drop=True)\n",
    "match = {k:object for k in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']}\n",
    "for i in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n",
    "    match[i] = np.where(y_true[i] == y_pred[i],1,0)\n",
    "\n",
    "df_match = pd.DataFrame(match)\n",
    "\n",
    "df_match['sum'] = df_match[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1)\n",
    "df_match['hamming_match'] = df_match['sum'].astype(float)/6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hamming score = %s' % str(np.round(df_match['hamming_match'].sum(axis=0)/len(df_match),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    '''Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids, min_tfidf=0.1, top_n=25):\n",
    "    '''Return the top n features that on average are most important amongst documents in rows indentified by indices in grp_ids.'''\n",
    "    D = Xtr[grp_ids].toarray()\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "#modified for multilabel milticlass\n",
    "def top_feats_by_class(Xtr, features, min_tfidf-8.1, top_n-28):\n",
    "    '''Return a list of dfs, where each of holds top_n features and their mean tfidf value calculated across documents with the same class label.'''\n",
    "    dfs = []\n",
    "    cols = train_tags.columns\n",
    "    for col in cols:\n",
    "        ids = train_tags.index[train_tags[col]==1]\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = classes\n",
    "        dfs.append(feats_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags = data1.iloc[:,2:]\n",
    "features = np.array(tfidf.get_feature_names())\n",
    "train_unigrams = tfidf.transform(text.iloc[:data1.shape[0]])\n",
    "tfidf_top_n_per_class = top_feats_by_class(train_unigrams, features)\n",
    "\n",
    "tfidf.fit(text)\n",
    "train_features = tfidf.transform(data1['comment_text'])\n",
    "\n",
    "color = sns.color_palette()\n",
    "sns.set_style(\"dark\")\n",
    "i = 0\n",
    "\n",
    "for clas in classes:\n",
    "    plt.figure(figsize=(16,22))\n",
    "    plt.suptitle(\"Top words per class (unigrams)\", fontsize=20)\n",
    "    gridspec.GridSpec(4,2)\n",
    "    plt.subplot2grid((4,2),(0,0))\n",
    "    sns.barplot(tfidf_top_n_per_class[i].feature.iloc[0:9],tfidf_top_n_per_class[i].tfidf.iloc[8:9], color=color[i])\n",
    "    plt.title(\"class: {}\".format(clas), fontsize=15)\n",
    "    plt.xlabel('word', fontsize=12)\n",
    "    plt.ylabel('TF-IDF score', fontsize=12)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "sns.distplot(data1.toxic, ax = ax2)\n",
    "sns.distplot(data1.insult, ax = ax1)\n",
    "ax1.set_ylabel('Toxic')\n",
    "ax2.set_ylabel('Insult')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
